{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genrate Train List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,json,csv\n",
    "\n",
    "trainRoot = \"/home/edward/test/OUTtest/CMU_AICenter/NIH_CXR_Images/train/\"\n",
    "testRoot = \"/home/edward/test/OUTtest/CMU_AICenter/NIH_CXR_Images/test/\"\n",
    "trainList='/home/edward/test/OUTtest/CMU_AICenter/trainReal1.json'\n",
    "fileDict ={\n",
    "    'train' : [],\n",
    "    'test' : []\n",
    "    }\n",
    "\n",
    "trainCsv = '/home/edward/test/OUTtest/CMU_AICenter/NIH_CXR_train.csv'\n",
    "testCsv = '/home/edward/test/OUTtest/CMU_AICenter/NIH_CXR_test.csv'\n",
    "\n",
    "\n",
    "trainLabelDict = {}\n",
    "testLabelDict = {}\n",
    "\n",
    "with open(trainCsv) as f:\n",
    "    rows = csv.reader(f)\n",
    "    for row in rows:\n",
    "#         print(row)\n",
    "        if('png' in row[0]):\n",
    "            trainLabelDict[row[0]]=row[1]\n",
    "# print(trainLabelDict)\n",
    "\n",
    "with open(testCsv) as f:\n",
    "    rows = csv.reader(f)\n",
    "    for row in rows:\n",
    "#         print(row)\n",
    "        if('png' in row[0]):\n",
    "            testLabelDict[row[0]]=row[1]\n",
    "# print(testLabelDict)\n",
    "\n",
    "\n",
    "for fp,dl,fl in os.walk(trainRoot):\n",
    "    # print(fp,dl,fl)\n",
    "    if(len(fl)>0):\n",
    "        for i in fl:\n",
    "            if(i.split('.')[-1]=='png'):\n",
    "                # print(fp + i)\n",
    "                if(trainLabelDict[i] == 'Atelectasis'):\n",
    "                    label = 1\n",
    "                if(trainLabelDict[i] == 'No Finding'):\n",
    "                    label = 0\n",
    "                # print(fp + '/' + i,label)\n",
    "                # with open(trainList,'a') as f:\n",
    "                #     f.writelines(fp + '/' + i + ',' + str(label) + '\\n')\n",
    "                fileDict['train'].append([fp + i,label])\n",
    "\n",
    "for fp,dl,fl in os.walk(testRoot):\n",
    "    # print(fp,dl,fl)\n",
    "    if(len(fl)>0):\n",
    "        for i in fl:\n",
    "            if(i.split('.')[-1]=='png'):\n",
    "                # print(fp + i)\n",
    "                if(testLabelDict[i] == 'Atelectasis'):\n",
    "                    label = 1\n",
    "                if(testLabelDict[i] == 'No Finding'):\n",
    "                    label = 0\n",
    "                # print(fp + '/' + i,label)\n",
    "                # with open(trainList,'a') as f:\n",
    "                #     f.writelines(fp + '/' + i + ',' + str(label) + '\\n')\n",
    "                fileDict['test'].append([fp + i,label])\n",
    "\n",
    "with open(trainList,'w') as f:\n",
    "    json.dump(fileDict,f,indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  1/424 [..............................] - ETA: 0s - loss: 0.4091 - accuracy: 0.1000 - precision: 0.1000 - recall: 0.1000 - auc: 0.1100WARNING:tensorflow:From /home/edward/.local/lib/python3.6/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "424/424 [==============================] - 111s 262ms/step - loss: 0.0558 - accuracy: 0.9151 - precision: 0.9151 - recall: 0.9151 - auc: 0.9266 - val_loss: 0.4311 - val_accuracy: 0.9069 - val_precision: 0.9069 - val_recall: 0.9069 - val_auc: 0.9063\n",
      "Epoch 2/30\n",
      "424/424 [==============================] - 110s 260ms/step - loss: 0.0416 - accuracy: 0.9283 - precision: 0.9283 - recall: 0.9283 - auc: 0.9413 - val_loss: 0.0426 - val_accuracy: 0.9103 - val_precision: 0.9103 - val_recall: 0.9103 - val_auc: 0.9131\n",
      "Epoch 3/30\n",
      "424/424 [==============================] - 110s 260ms/step - loss: 0.0362 - accuracy: 0.9370 - precision: 0.9370 - recall: 0.9370 - auc: 0.9396 - val_loss: 0.2949 - val_accuracy: 0.9069 - val_precision: 0.9069 - val_recall: 0.9069 - val_auc: 0.9111\n",
      "Epoch 4/30\n",
      "424/424 [==============================] - 111s 261ms/step - loss: 0.0350 - accuracy: 0.9361 - precision: 0.9361 - recall: 0.9361 - auc: 0.9450 - val_loss: 0.0419 - val_accuracy: 0.9138 - val_precision: 0.9138 - val_recall: 0.9138 - val_auc: 0.9179\n",
      "Epoch 5/30\n",
      "424/424 [==============================] - 111s 261ms/step - loss: 0.0344 - accuracy: 0.9392 - precision: 0.9392 - recall: 0.9392 - auc: 0.9417 - val_loss: 0.0404 - val_accuracy: 0.9138 - val_precision: 0.9138 - val_recall: 0.9138 - val_auc: 0.9138\n",
      "Epoch 6/30\n",
      "424/424 [==============================] - 111s 261ms/step - loss: 0.0342 - accuracy: 0.9392 - precision: 0.9392 - recall: 0.9392 - auc: 0.9435 - val_loss: 0.0510 - val_accuracy: 0.9069 - val_precision: 0.9069 - val_recall: 0.9069 - val_auc: 0.9115\n",
      "Epoch 7/30\n",
      "424/424 [==============================] - 111s 261ms/step - loss: 0.0343 - accuracy: 0.9392 - precision: 0.9392 - recall: 0.9392 - auc: 0.9423 - val_loss: 0.0423 - val_accuracy: 0.9069 - val_precision: 0.9069 - val_recall: 0.9069 - val_auc: 0.8915\n",
      "Epoch 8/30\n",
      "424/424 [==============================] - 111s 261ms/step - loss: 0.0346 - accuracy: 0.9403 - precision: 0.9403 - recall: 0.9403 - auc: 0.9401 - val_loss: 0.0432 - val_accuracy: 0.9069 - val_precision: 0.9069 - val_recall: 0.9069 - val_auc: 0.9135\n",
      "Epoch 9/30\n",
      "424/424 [==============================] - 111s 261ms/step - loss: 0.0337 - accuracy: 0.9408 - precision: 0.9408 - recall: 0.9408 - auc: 0.9445 - val_loss: 0.0476 - val_accuracy: 0.9069 - val_precision: 0.9069 - val_recall: 0.9069 - val_auc: 0.9056\n",
      "Epoch 10/30\n",
      "424/424 [==============================] - 111s 261ms/step - loss: 0.0332 - accuracy: 0.9406 - precision: 0.9406 - recall: 0.9406 - auc: 0.9413 - val_loss: 0.0467 - val_accuracy: 0.9069 - val_precision: 0.9069 - val_recall: 0.9069 - val_auc: 0.9072\n",
      "Epoch 11/30\n",
      "424/424 [==============================] - 111s 261ms/step - loss: 0.0334 - accuracy: 0.9406 - precision: 0.9406 - recall: 0.9406 - auc: 0.9410 - val_loss: 0.1543 - val_accuracy: 0.9069 - val_precision: 0.9069 - val_recall: 0.9069 - val_auc: 0.9066\n",
      "Epoch 12/30\n",
      "424/424 [==============================] - 111s 261ms/step - loss: 0.0374 - accuracy: 0.9366 - precision: 0.9366 - recall: 0.9366 - auc: 0.9437 - val_loss: 0.0431 - val_accuracy: 0.9103 - val_precision: 0.9103 - val_recall: 0.9103 - val_auc: 0.9142\n",
      "Epoch 13/30\n",
      "424/424 [==============================] - 111s 261ms/step - loss: 0.0336 - accuracy: 0.9403 - precision: 0.9403 - recall: 0.9403 - auc: 0.9469 - val_loss: 0.0581 - val_accuracy: 0.9138 - val_precision: 0.9138 - val_recall: 0.9138 - val_auc: 0.9283\n",
      "Epoch 14/30\n",
      "424/424 [==============================] - 111s 261ms/step - loss: 0.0329 - accuracy: 0.9387 - precision: 0.9387 - recall: 0.9387 - auc: 0.9488 - val_loss: 0.0747 - val_accuracy: 0.9069 - val_precision: 0.9069 - val_recall: 0.9069 - val_auc: 0.9049\n",
      "Epoch 15/30\n",
      "424/424 [==============================] - 111s 261ms/step - loss: 0.0323 - accuracy: 0.9410 - precision: 0.9410 - recall: 0.9410 - auc: 0.9475 - val_loss: 0.0585 - val_accuracy: 0.9069 - val_precision: 0.9069 - val_recall: 0.9069 - val_auc: 0.9094\n",
      "Epoch 16/30\n",
      "424/424 [==============================] - 111s 261ms/step - loss: 0.0324 - accuracy: 0.9403 - precision: 0.9403 - recall: 0.9403 - auc: 0.9512 - val_loss: 0.0410 - val_accuracy: 0.9138 - val_precision: 0.9138 - val_recall: 0.9138 - val_auc: 0.9166\n",
      "Epoch 17/30\n",
      "424/424 [==============================] - 111s 261ms/step - loss: 0.0321 - accuracy: 0.9410 - precision: 0.9410 - recall: 0.9410 - auc: 0.9513 - val_loss: 0.0426 - val_accuracy: 0.9103 - val_precision: 0.9103 - val_recall: 0.9103 - val_auc: 0.9211\n",
      "Epoch 18/30\n",
      "424/424 [==============================] - 111s 261ms/step - loss: 0.0326 - accuracy: 0.9394 - precision: 0.9394 - recall: 0.9394 - auc: 0.9484 - val_loss: 0.0476 - val_accuracy: 0.9069 - val_precision: 0.9069 - val_recall: 0.9069 - val_auc: 0.9187\n",
      "Epoch 19/30\n",
      "424/424 [==============================] - 111s 261ms/step - loss: 0.0349 - accuracy: 0.9389 - precision: 0.9389 - recall: 0.9389 - auc: 0.9497 - val_loss: 0.0599 - val_accuracy: 0.9103 - val_precision: 0.9103 - val_recall: 0.9103 - val_auc: 0.9286\n",
      "Epoch 20/30\n",
      "424/424 [==============================] - 111s 261ms/step - loss: 0.0329 - accuracy: 0.9401 - precision: 0.9401 - recall: 0.9401 - auc: 0.9500 - val_loss: 0.0451 - val_accuracy: 0.9069 - val_precision: 0.9069 - val_recall: 0.9069 - val_auc: 0.9278\n",
      "Epoch 21/30\n",
      "424/424 [==============================] - 110s 260ms/step - loss: 0.0320 - accuracy: 0.9410 - precision: 0.9410 - recall: 0.9410 - auc: 0.9517 - val_loss: 0.0551 - val_accuracy: 0.9069 - val_precision: 0.9069 - val_recall: 0.9069 - val_auc: 0.9179\n",
      "Epoch 22/30\n",
      "424/424 [==============================] - 110s 261ms/step - loss: 0.0318 - accuracy: 0.9406 - precision: 0.9406 - recall: 0.9406 - auc: 0.9537 - val_loss: 0.0633 - val_accuracy: 0.8241 - val_precision: 0.8241 - val_recall: 0.8241 - val_auc: 0.8937\n",
      "Epoch 23/30\n",
      "424/424 [==============================] - 110s 260ms/step - loss: 0.0319 - accuracy: 0.9406 - precision: 0.9406 - recall: 0.9406 - auc: 0.9528 - val_loss: 0.0434 - val_accuracy: 0.9069 - val_precision: 0.9069 - val_recall: 0.9069 - val_auc: 0.9097\n",
      "Epoch 24/30\n",
      "424/424 [==============================] - 111s 261ms/step - loss: 0.0337 - accuracy: 0.9406 - precision: 0.9406 - recall: 0.9406 - auc: 0.9489 - val_loss: 0.2258 - val_accuracy: 0.9069 - val_precision: 0.9069 - val_recall: 0.9069 - val_auc: 0.9079\n",
      "Epoch 25/30\n",
      "424/424 [==============================] - 110s 260ms/step - loss: 0.0327 - accuracy: 0.9403 - precision: 0.9403 - recall: 0.9403 - auc: 0.9499 - val_loss: 0.0487 - val_accuracy: 0.9103 - val_precision: 0.9103 - val_recall: 0.9103 - val_auc: 0.9216\n",
      "Epoch 26/30\n",
      "424/424 [==============================] - 110s 260ms/step - loss: 0.0317 - accuracy: 0.9413 - precision: 0.9413 - recall: 0.9413 - auc: 0.9515 - val_loss: 0.0452 - val_accuracy: 0.9103 - val_precision: 0.9103 - val_recall: 0.9103 - val_auc: 0.9175\n",
      "Epoch 27/30\n",
      "424/424 [==============================] - 110s 261ms/step - loss: 0.0324 - accuracy: 0.9403 - precision: 0.9403 - recall: 0.9403 - auc: 0.9502 - val_loss: 0.0407 - val_accuracy: 0.9103 - val_precision: 0.9103 - val_recall: 0.9103 - val_auc: 0.9227\n",
      "Epoch 28/30\n",
      "424/424 [==============================] - 110s 261ms/step - loss: 0.0313 - accuracy: 0.9410 - precision: 0.9410 - recall: 0.9410 - auc: 0.9549 - val_loss: 0.0425 - val_accuracy: 0.9069 - val_precision: 0.9069 - val_recall: 0.9069 - val_auc: 0.8984\n",
      "Epoch 29/30\n",
      "424/424 [==============================] - 110s 260ms/step - loss: 0.0324 - accuracy: 0.9413 - precision: 0.9413 - recall: 0.9413 - auc: 0.9570 - val_loss: 0.0419 - val_accuracy: 0.9103 - val_precision: 0.9103 - val_recall: 0.9103 - val_auc: 0.8911\n",
      "Epoch 30/30\n",
      "424/424 [==============================] - 110s 261ms/step - loss: 0.0336 - accuracy: 0.9415 - precision: 0.9415 - recall: 0.9415 - auc: 0.9581 - val_loss: 0.0396 - val_accuracy: 0.9138 - val_precision: 0.9138 - val_recall: 0.9138 - val_auc: 0.9185\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input\n",
    "from keras import backend as K\n",
    "import os, datetime, tensorflow as tf, numpy as np,cv2,json\n",
    "\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "# from tensorflow.python.client import device_lib\n",
    "# print(device_lib.list_local_devices())\n",
    "# print(tf.__version__)\n",
    "# print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "# tf.test.is_gpu_available()\n",
    "\n",
    "def binary_focal_loss(gamma=2, alpha=0.25):\n",
    "    alpha = tf.constant(alpha, dtype=tf.float32)\n",
    "    gamma = tf.constant(gamma, dtype=tf.float32)\n",
    "\n",
    "    def binary_focal_loss_fixed(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        alpha_t = y_true*alpha + (K.ones_like(y_true)-y_true)*(1-alpha)\n",
    "    \n",
    "        p_t = y_true*y_pred + (K.ones_like(y_true)-y_true)*(K.ones_like(y_true)-y_pred) + K.epsilon()\n",
    "        focal_loss = - alpha_t * K.pow((K.ones_like(y_true)-p_t),gamma) * K.log(p_t)\n",
    "        return K.mean(focal_loss)\n",
    "    return binary_focal_loss_fixed\n",
    "\n",
    "class datagenator(tf.keras.utils.Sequence):\n",
    "\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, batch_size=10, shuffle=True,n_classes = 2,mode='train'):\n",
    "        'Initialization'\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.shuffle = shuffle\n",
    "        self.n_classes = n_classes\n",
    "        self.mode = mode\n",
    "        self.err={\n",
    "            'effective':[],\n",
    "            'invalid':[]\n",
    "        }\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "        # print(80*'!')\n",
    "        # print(np.shape(X),np.shape(y))\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = [] #np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = [] #np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for ID in list_IDs_temp:\n",
    "            # Store sample\n",
    "            img = cv2.imread(ID)\n",
    "            in_ = np.array(img, dtype=np.float32)\n",
    "            shape=np.shape(in_)\n",
    "            try:\n",
    "                side = 517\n",
    "                if(shape[0]>shape[1]):\n",
    "                        in_=cv2.resize(in_,(side,shape[1]*side/shape[0]),interpolation=cv2.INTER_CUBIC)\n",
    "                        newshape=np.shape(in_)\n",
    "                        addsp=(side-newshape[0])/2\n",
    "                        fr_sp=np.zeros((addsp,side,3),np.float32)\n",
    "                        if((side-newshape[0])%2!=0):\n",
    "                                addsp+=1\n",
    "                        bk_sp=np.zeros((addsp,side,3),np.float32)\n",
    "                        in_=np.concatenate((fr_sp,in_,bk_sp),axis=0)\n",
    "                elif(shape[0]<shape[1]):\n",
    "                        in_=cv2.resize(in_,(shape[0]*side/shape[1],side),interpolation=cv2.INTER_CUBIC)\n",
    "                        newshape=np.shape(in_)\n",
    "                        addsp=(side-newshape[1])/2\n",
    "                        fr_sp=np.zeros((side,addsp,3),np.float32)\n",
    "                        if((side-newshape[1])%2!=0):\n",
    "                                addsp+=1\n",
    "                        bk_sp=np.zeros((side,addsp,3),np.float32)\n",
    "                        in_=np.concatenate((fr_sp,in_,bk_sp),axis=1)\n",
    "\n",
    "                else:\n",
    "                        in_=cv2.resize(in_,(side,side),interpolation=cv2.INTER_CUBIC)\n",
    "            except:\n",
    "                in_ = np.array(np.random.randint(0,256,size = (517,517,3)), dtype=np.float32)\n",
    "                # print(ID)\n",
    "                if('effective' in ID):\n",
    "                    if not(ID.split('effective')[-1].split('/')[1] in self.err['effective']):\n",
    "                        self.err['effective'].append(ID.split('effective')[-1].split('/')[1])\n",
    "                        self.err['effective'].sort()\n",
    "                if('invalid' in ID):\n",
    "                    if not(ID.split('invalid')[-1].split('/')[1] in self.err['invalid']):\n",
    "                        self.err['invalid'].append(ID.split('invalid')[-1].split('/')[1])\n",
    "                        self.err['invalid'].sort()\n",
    "\n",
    "                if(self.mode =='train'):\n",
    "                    with open('err_train.json','w') as f:\n",
    "                        # f.writelines(ID + '\\n')\n",
    "                        json.dump(self.err,f,indent=4)\n",
    "                \n",
    "                if(self.mode =='val'):\n",
    "                    with open('err_test.json','w') as f:\n",
    "                        # f.writelines(ID + '\\n')\n",
    "                        json.dump(self.err,f,indent=4)\n",
    "            \n",
    "            X.append(in_)\n",
    "\n",
    "            # Store class\n",
    "            y.append(self.labels[ID])\n",
    "\n",
    "        return np.array(X), np.array(to_categorical(y, num_classes=self.n_classes))\n",
    "\n",
    "\n",
    "imgList = json.load(open('/home/edward/test/OUTtest/CMU_AICenter/trainReal1.json'))\n",
    "\n",
    "partition = {\n",
    "    'train' :[],\n",
    "    'test' :[]\n",
    "}\n",
    "labels = {}\n",
    "\n",
    "for i in imgList['train']:\n",
    "    partition['train'].append(i[0])\n",
    "    labels[i[0]] = i[1]\n",
    "for i in imgList['test']:\n",
    "    partition['test'].append(i[0])\n",
    "    labels[i[0]] = i[1]\n",
    "\n",
    "# this could also be the output a different Keras model or layer\n",
    "input_tensor = Input(shape=(517, 517, 3))\n",
    "# create the base pre-trained model\n",
    "base_model = InceptionV3(   \n",
    "    input_tensor=input_tensor,\n",
    "    weights=None,\n",
    "    include_top=True,\n",
    "    classes=2,\n",
    ") #input_tensor=input_tensor, weights='imagenet', \n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "# x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "# x = Dense(1024, activation='relu')(x)\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "# predictions = Dense(2, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=x)\n",
    "# model.summary()\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer='rmsprop', loss=[binary_focal_loss(alpha=.25, gamma=2)], metrics=['accuracy', 'Precision', 'Recall', 'AUC']) \n",
    "#'categorical_crossentropy' [binary_focal_loss(alpha=.25, gamma=2)]\n",
    "\n",
    "# model.load_weights(\"/home/cwlab/edward/keras_test/CODE/checkpoint/bk/weights.11-1.00.hdf5\", by_name=True)\n",
    "\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "checkPointPath = './checkpoint/weights.{epoch:02d}-{auc:.2f}.hdf5'\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(checkPointPath, monitor='auc', save_weights_only=True)\n",
    "\n",
    "training_generator = datagenator(partition['train'], labels, batch_size=10, n_classes = 2, shuffle=True, mode='train')\n",
    "val__generator = datagenator(partition['test'], labels, batch_size=10, n_classes = 2, shuffle=True,mode='val')\n",
    "# x_train = np.random.randint(0,256,size=(1000,517,517,3))\n",
    "# print(np.shape(x_train))\n",
    "# y_train=to_categorical(np.random.randint(0,2,size=(1000)))\n",
    "# print('y_train= ',y_train)\n",
    "# print([[0.,1.]])\n",
    "\n",
    "model.fit(\n",
    "    x = training_generator, \n",
    "    epochs = 30,\n",
    "    callbacks = [tensorboard_callback, checkpoint_callback],\n",
    "    validation_data = val__generator\n",
    "    ) #validation_data=(x_test, y_test),batch_size = 10, \n",
    "\n",
    "log_writer = tf.summary.create_file_writer(logdir)\n",
    "\n",
    "model.save_weights(os.path.join(\"weights\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + '.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 4s 123ms/step - loss: 0.4382 - accuracy: 0.9069 - precision: 0.9069 - recall: 0.9069 - auc: 0.9078\n",
      "{'loss': 0.43824172019958496, 'accuracy': 0.9068965315818787, 'precision': 0.9068965315818787, 'recall': 0.9068965315818787, 'auc': 0.9078359603881836}\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "# from tensorflowf.keras.utils import Sequence\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.models import Model\n",
    "from keras import backend as K\n",
    "import numpy as np,cv2,tensorflow as tf,json,os,datetime\n",
    "\n",
    "# y=np.random.randint(0,4,size=(1,512,512,3))\n",
    "# y_test=to_categorical(y)\n",
    "# print(y_test)\n",
    "\n",
    "def binary_focal_loss(gamma=2, alpha=0.25):\n",
    "    alpha = tf.constant(alpha, dtype=tf.float32)\n",
    "    gamma = tf.constant(gamma, dtype=tf.float32)\n",
    "\n",
    "    def binary_focal_loss_fixed(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        alpha_t = y_true*alpha + (K.ones_like(y_true)-y_true)*(1-alpha)\n",
    "    \n",
    "        p_t = y_true*y_pred + (K.ones_like(y_true)-y_true)*(K.ones_like(y_true)-y_pred) + K.epsilon()\n",
    "        focal_loss = - alpha_t * K.pow((K.ones_like(y_true)-p_t),gamma) * K.log(p_t)\n",
    "        return K.mean(focal_loss)\n",
    "    return binary_focal_loss_fixed\n",
    "\n",
    "class datagenator(tf.keras.utils.Sequence):\n",
    "\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, batch_size=10, shuffle=True,n_classes = 10,mode='train'):\n",
    "        'Initialization'\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.shuffle = shuffle\n",
    "        self.n_classes = n_classes\n",
    "        self.mode = mode\n",
    "        self.err={\n",
    "            'effective':[],\n",
    "            'invalid':[]\n",
    "        }\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "#         print(len(self.list_IDs))\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "        # print(80*'!')\n",
    "        # print(np.shape(X),np.shape(y))\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = [] #np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = [] #np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for ID in list_IDs_temp:\n",
    "            # Store sample\n",
    "            img = cv2.imread(ID)\n",
    "            in_ = np.array(img, dtype=np.float32)\n",
    "            shape=np.shape(in_)\n",
    "            try:\n",
    "                side = 517\n",
    "                if(shape[0]>shape[1]):\n",
    "                        in_=cv2.resize(in_,(side,shape[1]*side/shape[0]),interpolation=cv2.INTER_CUBIC)\n",
    "                        newshape=np.shape(in_)\n",
    "                        addsp=(side-newshape[0])/2\n",
    "                        fr_sp=np.zeros((addsp,side,3),np.float32)\n",
    "                        if((side-newshape[0])%2!=0):\n",
    "                                addsp+=1\n",
    "                        bk_sp=np.zeros((addsp,side,3),np.float32)\n",
    "                        in_=np.concatenate((fr_sp,in_,bk_sp),axis=0)\n",
    "                elif(shape[0]<shape[1]):\n",
    "                        in_=cv2.resize(in_,(shape[0]*side/shape[1],side),interpolation=cv2.INTER_CUBIC)\n",
    "                        newshape=np.shape(in_)\n",
    "                        addsp=(side-newshape[1])/2\n",
    "                        fr_sp=np.zeros((side,addsp,3),np.float32)\n",
    "                        if((side-newshape[1])%2!=0):\n",
    "                                addsp+=1\n",
    "                        bk_sp=np.zeros((side,addsp,3),np.float32)\n",
    "                        in_=np.concatenate((fr_sp,in_,bk_sp),axis=1)\n",
    "\n",
    "                else:\n",
    "                        in_=cv2.resize(in_,(side,side),interpolation=cv2.INTER_CUBIC)\n",
    "            except:\n",
    "                in_ = np.array(np.random.randint(0,256,size = (517,517,3)), dtype=np.float32)\n",
    "                # print(ID)\n",
    "                if('effective' in ID):\n",
    "                    if not(ID.split('effective')[-1].split('/')[1] in self.err['effective']):\n",
    "                        self.err['effective'].append(ID.split('effective')[-1].split('/')[1])\n",
    "                        self.err['effective'].sort()\n",
    "                if('invalid' in ID):\n",
    "                    if not(ID.split('invalid')[-1].split('/')[1] in self.err['invalid']):\n",
    "                        self.err['invalid'].append(ID.split('invalid')[-1].split('/')[1])\n",
    "                        self.err['invalid'].sort()\n",
    "\n",
    "                if(self.mode =='train'):\n",
    "                    with open('err_train.json','w') as f:\n",
    "                        # f.writelines(ID + '\\n')\n",
    "                        json.dump(self.err,f,indent=4)\n",
    "                \n",
    "                if(self.mode =='val'):\n",
    "                    with open('err_test.json','w') as f:\n",
    "                        # f.writelines(ID + '\\n')\n",
    "                        json.dump(self.err,f,indent=4)\n",
    "            \n",
    "            X.append(in_)\n",
    "\n",
    "            # Store class\n",
    "            y.append(self.labels[ID])\n",
    "\n",
    "        return np.array(X), np.array(to_categorical(y, num_classes=self.n_classes))\n",
    "\n",
    "imgList = json.load(open('/home/edward/test/OUTtest/CMU_AICenter/trainReal1.json'))\n",
    "\n",
    "partition = {\n",
    "    'test' :[]\n",
    "}\n",
    "labels = {}\n",
    "\n",
    "# print(len(imgList['test']))\n",
    "for i in imgList['test']:\n",
    "    partition['test'].append(i[0])\n",
    "    labels[i[0]] = i[1]\n",
    "\n",
    "val__generator = datagenator(partition['test'], labels, batch_size=10, n_classes = 2, shuffle=True, mode='val')\n",
    "\n",
    "input_tensor = Input(shape=(517, 517, 3))\n",
    "base_model = InceptionV3(   \n",
    "    input_tensor=input_tensor,\n",
    "    weights=None,\n",
    "    include_top=True,\n",
    "    classes=2,\n",
    ")\n",
    "x = base_model.output\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=x)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy', 'Precision', 'Recall', 'AUC'])\n",
    "#'categorical_crossentropy' [binary_focal_loss(alpha=.25, gamma=2)] \n",
    "\n",
    "model.load_weights(\"/home/edward/test/OUTtest/CMU_AICenter/checkpoint/weights.30-0.96.hdf5\", by_name=True)\n",
    "\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#==============evaluate======================\n",
    "evalValue = model.evaluate(\n",
    "    x=val__generator,\n",
    "    callbacks=tensorboard_callback,\n",
    "    return_dict=True,\n",
    ")\n",
    "print(evalValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSprop + categorical_crossentropy\n",
    "28\n",
    "{'loss': 0.32754281163215637, 'accuracy': 0.9068965315818787, 'precision': 0.9068965315818787, 'recall': 0.9068965315818787, 'auc': 0.9135434031486511}\n",
    "\n",
    "### ADAM + categorical_crossentropy\n",
    "25\n",
    "{'loss': 0.32302114367485046, 'accuracy': 0.9103448390960693, 'precision': 0.9103448390960693, 'recall': 0.9103448390960693, 'auc': 0.9054222106933594}\n",
    "\n",
    "### ADAM + Focal Loss\n",
    "22\n",
    "{'loss': 0.06994660943746567, 'accuracy': 0.9068965315818787, 'precision': 0.9068965315818787, 'recall': 0.9068965315818787, 'auc': 0.9103567004203796}\n",
    "\n",
    "### RMSprop + Focal Loss\n",
    "30\n",
    "{'loss': 0.43824172019958496, 'accuracy': 0.9068965315818787, 'precision': 0.9068965315818787, 'recall': 0.9068965315818787, 'auc': 0.9078359603881836}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
