{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split to folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,json,csv,shutil\n",
    "\n",
    "trainRoot = \"/home/edward/test/OUTtest/CMU_AICenter/NIH_CXR_Images/train/\"\n",
    "testRoot = \"/home/edward/test/OUTtest/CMU_AICenter/NIH_CXR_Images/test/\"\n",
    "trainCsv = '/home/edward/test/OUTtest/CMU_AICenter/NIH_CXR_train.csv'\n",
    "testCsv = '/home/edward/test/OUTtest/CMU_AICenter/NIH_CXR_test.csv'\n",
    "\n",
    "trainLabelDict = {}\n",
    "testLabelDict = {}\n",
    "\n",
    "with open(trainCsv) as f:\n",
    "    rows = csv.reader(f)\n",
    "    for row in rows:\n",
    "#         print(row)\n",
    "        if('png' in row[0]):\n",
    "            trainLabelDict[row[0]]=row[1]\n",
    "# print(trainLabelDict)\n",
    "\n",
    "with open(testCsv) as f:\n",
    "    rows = csv.reader(f)\n",
    "    for row in rows:\n",
    "#         print(row)\n",
    "        if('png' in row[0]):\n",
    "            testLabelDict[row[0]]=row[1]\n",
    "# print(testLabelDict)\n",
    "\n",
    "for fp,dl,fl in os.walk(trainRoot):\n",
    "    # print(fp,dl,fl)\n",
    "    if(len(fl)>0):\n",
    "        for i in fl:\n",
    "            if(i.split('.')[-1]=='png'):\n",
    "                # print(fp + i)\n",
    "                if(trainLabelDict[i] == 'Atelectasis'):\n",
    "#                     label = 1\n",
    "                    if not(os.path.isdir('./Split_Folder/train/Atelectasis')):\n",
    "                        os.makedirs('./Split_Folder/train/Atelectasis')\n",
    "                    shutil.copy(fp + '/' + i, './Split_Folder/train/Atelectasis/' + i)\n",
    "                if(trainLabelDict[i] == 'No Finding'):\n",
    "#                     label = 0\n",
    "                    if not(os.path.isdir('./Split_Folder/train/No Finding')):\n",
    "                        os.makedirs('./Split_Folder/train/No Finding')\n",
    "                    shutil.copy(fp + '/' + i, './Split_Folder/train/No Finding/' + i)\n",
    "                # print(fp + '/' + i,label)\n",
    "                # with open(trainList,'a') as f:\n",
    "                #     f.writelines(fp + '/' + i + ',' + str(label) + '\\n')\n",
    "#                 fileDict['train'].append([fp + i,label])\n",
    "\n",
    "for fp,dl,fl in os.walk(testRoot):\n",
    "    # print(fp,dl,fl)\n",
    "    if(len(fl)>0):\n",
    "        for i in fl:\n",
    "            if(i.split('.')[-1]=='png'):\n",
    "                # print(fp + i)\n",
    "                if(testLabelDict[i] == 'Atelectasis'):\n",
    "#                     label = 1\n",
    "                    if not(os.path.isdir('./Split_Folder/test/Atelectasis')):\n",
    "                        os.makedirs('./Split_Folder/test/Atelectasis')\n",
    "                    shutil.copy(fp + '/' + i, './Split_Folder/test/Atelectasis/' + i)\n",
    "                if(testLabelDict[i] == 'No Finding'):\n",
    "#                     label = 0\n",
    "                    if not(os.path.isdir('./Split_Folder/test/No Finding')):\n",
    "                        os.makedirs('./Split_Folder/test/No Finding')\n",
    "                    shutil.copy(fp + '/' + i, './Split_Folder/test/No Finding/' + i)\n",
    "                # print(fp + '/' + i,label)\n",
    "                # with open(trainList,'a') as f:\n",
    "                #     f.writelines(fp + '/' + i + ',' + str(label) + '\\n')\n",
    "#                 fileDict['test'].append([fp + i,label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genrate Train List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3307 3993 27 270\n"
     ]
    }
   ],
   "source": [
    "import os,json,csv\n",
    "\n",
    "trainRoot = \"/home/edward/test/OUTtest/CMU_AICenter/Split_Folder/train/\"\n",
    "testRoot = \"/home/edward/test/OUTtest/CMU_AICenter/Split_Folder/test/\"\n",
    "trainList='/home/edward/test/OUTtest/CMU_AICenter/trainReal1.json'\n",
    "fileDict ={\n",
    "    'train' : [],\n",
    "    'test' : []\n",
    "    }\n",
    "\n",
    "tra=0\n",
    "trn=0\n",
    "tsa=0\n",
    "tsn=0\n",
    "for fp,dl,fl in os.walk(trainRoot):\n",
    "    # print(fp,dl,fl)\n",
    "    if(len(fl)>0):\n",
    "        for i in fl:\n",
    "            if(i.split('.')[-1]=='png'):\n",
    "                # print(fp + i)\n",
    "                if(fp.split(trainRoot)[-1].split('/')[0] == 'Atelectasis'):\n",
    "                    label = 1\n",
    "                    tra += 1\n",
    "                if(fp.split(trainRoot)[-1].split('/')[0] == 'No Finding'):\n",
    "                    label = 0\n",
    "                    trn += 1\n",
    "                # print(fp + '/' + i,label)\n",
    "                # with open(trainList,'a') as f:\n",
    "                #     f.writelines(fp + '/' + i + ',' + str(label) + '\\n')\n",
    "                fileDict['train'].append([fp + '/' + i,label])\n",
    "\n",
    "for fp,dl,fl in os.walk(testRoot):\n",
    "    # print(fp,dl,fl)\n",
    "    if(len(fl)>0):\n",
    "        for i in fl:\n",
    "            if(i.split('.')[-1]=='png'):\n",
    "                # print(fp + i)\n",
    "                if(fp.split(testRoot)[-1].split('/')[0] == 'Atelectasis'):\n",
    "                    label = 1\n",
    "                    tsa += 1\n",
    "                if(fp.split(testRoot)[-1].split('/')[0] == 'No Finding'):\n",
    "                    label = 0\n",
    "                    tsn += 1\n",
    "                # print(fp + '/' + i,label)\n",
    "                # with open(trainList,'a') as f:\n",
    "                #     f.writelines(fp + '/' + i + ',' + str(label) + '\\n')\n",
    "                fileDict['test'].append([fp + '/' + i,label])\n",
    "\n",
    "print(tra,trn,tsa,tsn)\n",
    "\n",
    "with open(trainList,'w') as f:\n",
    "    json.dump(fileDict,f,indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampling \"No Finding\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,shutil,random\n",
    "\n",
    "oldNFPath = \"/home/edward/test/OUTtest/CMU_AICenter/Split_Folder/train_no/\"\n",
    "newNFPath = \"/home/edward/test/OUTtest/CMU_AICenter/Split_Folder/train/No Finding/\"\n",
    "\n",
    "flist = os.listdir(oldNFPath)\n",
    "# print(flist)\n",
    "# print(80*'x')\n",
    "random.shuffle(flist)\n",
    "# print(flist)\n",
    "sample = 251\n",
    "for i in range(sample):\n",
    "    shutil.move(oldNFPath + flist[i], newNFPath + flist[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import  Conv2D, BatchNormalization, Activation, Input, AveragePooling2D, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.utils import to_categorical\n",
    "# from tensorflow.keras.layers import Input\n",
    "from keras import backend as K\n",
    "import os, datetime, tensorflow as tf, numpy as np,cv2,json\n",
    "\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4935820895522388]\n",
      "[0.5959701492537314]\n"
     ]
    }
   ],
   "source": [
    "freq_pos = [3307/6700]\n",
    "freq_neg = [3993/6700]\n",
    "print(freq_pos)\n",
    "print(freq_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_pos = K.constant(freq_neg,dtype='float32')\n",
    "weights_neg = K.constant(freq_pos,dtype='float32')\n",
    "epsilon=1e-7\n",
    "def weighted_loss(y_true, y_pred):\n",
    "    loss = 0.0\n",
    "    loss_pos = -1 * K.sum( weights_pos * y_true * K.log(y_pred + epsilon), axis=-1)\n",
    "    loss_neg = -1 * K.sum( weights_neg * (1 - y_true) * K.log(1 - y_pred + epsilon) ,axis=-1)\n",
    "    return (loss_pos+loss_neg)/2\n",
    "\n",
    "def binary_focal_loss(gamma=2, alpha=0.25):\n",
    "    alpha = tf.constant(alpha, dtype=tf.float32)\n",
    "    gamma = tf.constant(gamma, dtype=tf.float32)\n",
    "\n",
    "    def binary_focal_loss_fixed(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        alpha_t = y_true*alpha + (K.ones_like(y_true)-y_true)*(1-alpha)\n",
    "    \n",
    "        p_t = y_true*y_pred + (K.ones_like(y_true)-y_true)*(K.ones_like(y_true)-y_pred) + K.epsilon()\n",
    "        focal_loss = - alpha_t * K.pow((K.ones_like(y_true)-p_t),gamma) * K.log(p_t)\n",
    "        return K.mean(focal_loss)\n",
    "    return binary_focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_layer(inputs,\n",
    "                 num_filters=16,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 batch_normalization=True,\n",
    "                 conv_first=True):\n",
    "    \n",
    "    conv = Conv2D(num_filters,\n",
    "                  kernel_size=kernel_size,\n",
    "                  strides=strides,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))\n",
    "\n",
    "    x = inputs\n",
    "    if conv_first:\n",
    "        x = conv(x)\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "    else:\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "        x = conv(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_v2(input_shape, depth,num_classes):\n",
    "    if (depth - 2) % 9 != 0:\n",
    "        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
    "    # Start model definition.\n",
    "    num_filters_in = 16\n",
    "    num_res_blocks = int((depth - 2) / 9)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
    "    x = resnet_layer(inputs=inputs,\n",
    "                     num_filters=num_filters_in,\n",
    "                     conv_first=True)\n",
    "\n",
    "    # Instantiate the stack of residual units\n",
    "    for stage in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            activation = 'relu'\n",
    "            batch_normalization = True\n",
    "            strides = 1\n",
    "            if stage == 0:\n",
    "                num_filters_out = num_filters_in * 4\n",
    "                if res_block == 0:  # first layer and first stage\n",
    "                    activation = None\n",
    "                    batch_normalization = False\n",
    "            else:\n",
    "                num_filters_out = num_filters_in * 2\n",
    "                if res_block == 0:  # first layer but not first stage\n",
    "                    strides = 2    # downsample\n",
    "\n",
    "            # bottleneck residual unit\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters_in,\n",
    "                             kernel_size=1,\n",
    "                             strides=strides,\n",
    "                             activation=activation,\n",
    "                             batch_normalization=batch_normalization,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_in,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_out,\n",
    "                             kernel_size=1,\n",
    "                             conv_first=False)\n",
    "            if res_block == 0:\n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters_out,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = tf.keras.layers.add([x, y])\n",
    "\n",
    "        num_filters_in = num_filters_out\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v2 has BN-ReLU before Pooling\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu',name='last_activation_layer')(x)\n",
    "    x = AveragePooling2D(pool_size=8)(x)\n",
    "    y = Flatten()(x)\n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='sigmoid',\n",
    "                    kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # Instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  1/146 [..............................] - ETA: 0s - loss: 1.9224 - accuracy: 0.4600 - precision: 0.4600 - recall: 0.4600 - auc: 0.4156WARNING:tensorflow:From /home/edward/.local/lib/python3.6/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "146/146 [==============================] - 93s 638ms/step - loss: 1.4171 - accuracy: 0.7793 - precision: 0.7311 - recall: 0.7885 - auc: 0.8180 - val_loss: 3.3549 - val_accuracy: 0.1145 - val_precision: 0.1261 - val_recall: 0.0505 - val_auc: 0.1160\n",
      "Epoch 2/100\n",
      "146/146 [==============================] - 92s 628ms/step - loss: 1.0308 - accuracy: 0.8314 - precision: 0.7957 - recall: 0.7837 - auc: 0.8637 - val_loss: 2.9177 - val_accuracy: 0.1010 - val_precision: 0.1635 - val_recall: 0.0572 - val_auc: 0.1149\n",
      "Epoch 3/100\n",
      "146/146 [==============================] - 91s 625ms/step - loss: 0.8277 - accuracy: 0.8604 - precision: 0.8295 - recall: 0.8077 - auc: 0.8947 - val_loss: 0.9202 - val_accuracy: 0.8215 - val_precision: 0.9130 - val_recall: 0.2121 - val_auc: 0.8653\n",
      "Epoch 4/100\n",
      "146/146 [==============================] - 91s 626ms/step - loss: 0.6734 - accuracy: 0.9047 - precision: 0.9080 - recall: 0.7696 - auc: 0.9418 - val_loss: 1.1241 - val_accuracy: 0.8249 - val_precision: 0.8882 - val_recall: 0.4815 - val_auc: 0.8558\n",
      "Epoch 5/100\n",
      "146/146 [==============================] - 91s 625ms/step - loss: 0.5794 - accuracy: 0.9248 - precision: 0.9464 - recall: 0.7566 - auc: 0.9611 - val_loss: 1.3557 - val_accuracy: 0.5859 - val_precision: 0.5294 - val_recall: 0.0606 - val_auc: 0.5963\n",
      "Epoch 6/100\n",
      "146/146 [==============================] - 91s 624ms/step - loss: 0.5021 - accuracy: 0.9458 - precision: 0.9761 - recall: 0.7490 - auc: 0.9788 - val_loss: 0.9976 - val_accuracy: 0.7239 - val_precision: 0.8537 - val_recall: 0.1178 - val_auc: 0.7674\n",
      "Epoch 7/100\n",
      "146/146 [==============================] - 91s 624ms/step - loss: 0.4392 - accuracy: 0.9634 - precision: 0.9864 - recall: 0.6681 - auc: 0.9853 - val_loss: 3.7553 - val_accuracy: 0.1751 - val_precision: 0.1739 - val_recall: 0.0135 - val_auc: 0.1344\n",
      "Epoch 8/100\n",
      "146/146 [==============================] - 91s 624ms/step - loss: 0.3980 - accuracy: 0.9715 - precision: 0.9924 - recall: 0.6786 - auc: 0.9900 - val_loss: 0.8455 - val_accuracy: 0.8754 - val_precision: 0.8824 - val_recall: 0.2020 - val_auc: 0.8939\n",
      "Epoch 9/100\n",
      "146/146 [==============================] - 91s 624ms/step - loss: 0.3784 - accuracy: 0.9718 - precision: 0.9955 - recall: 0.6651 - auc: 0.9918 - val_loss: 0.9278 - val_accuracy: 0.7710 - val_precision: 0.7500 - val_recall: 0.0202 - val_auc: 0.8239\n",
      "Epoch 10/100\n",
      "146/146 [==============================] - 91s 625ms/step - loss: 0.3494 - accuracy: 0.9796 - precision: 0.9974 - recall: 0.6199 - auc: 0.9941 - val_loss: 0.8247 - val_accuracy: 0.8384 - val_precision: 0.7857 - val_recall: 0.0370 - val_auc: 0.8804\n",
      "Epoch 11/100\n",
      "146/146 [==============================] - 91s 624ms/step - loss: 0.3197 - accuracy: 0.9879 - precision: 0.9993 - recall: 0.5692 - auc: 0.9954 - val_loss: 2.3027 - val_accuracy: 0.3906 - val_precision: 1.0000 - val_recall: 0.0135 - val_auc: 0.3902\n",
      "Epoch 12/100\n",
      "146/146 [==============================] - 91s 623ms/step - loss: 0.3338 - accuracy: 0.9774 - precision: 0.9990 - recall: 0.5422 - auc: 0.9920 - val_loss: 0.7933 - val_accuracy: 0.8620 - val_precision: 0.8667 - val_recall: 0.0438 - val_auc: 0.8746\n",
      "Epoch 13/100\n",
      "146/146 [==============================] - 91s 622ms/step - loss: 0.3018 - accuracy: 0.9885 - precision: 0.9998 - recall: 0.5899 - auc: 0.9961 - val_loss: 0.8479 - val_accuracy: 0.8923 - val_precision: 0.8571 - val_recall: 0.0808 - val_auc: 0.8958\n",
      "Epoch 14/100\n",
      "146/146 [==============================] - 91s 620ms/step - loss: 0.2790 - accuracy: 0.9934 - precision: 0.9995 - recall: 0.5577 - auc: 0.9965 - val_loss: 1.3539 - val_accuracy: 0.6667 - val_precision: 0.7143 - val_recall: 0.0168 - val_auc: 0.6524\n",
      "Epoch 15/100\n",
      "146/146 [==============================] - 90s 619ms/step - loss: 0.2628 - accuracy: 0.9942 - precision: 1.0000 - recall: 0.5260 - auc: 0.9958 - val_loss: 1.5303 - val_accuracy: 0.5522 - val_precision: 0.3333 - val_recall: 0.0067 - val_auc: 0.5593\n",
      "Epoch 16/100\n",
      "146/146 [==============================] - 90s 619ms/step - loss: 0.3123 - accuracy: 0.9738 - precision: 0.9989 - recall: 0.5158 - auc: 0.9856 - val_loss: 8.8587 - val_accuracy: 0.0976 - val_precision: 0.0871 - val_recall: 0.0707 - val_auc: 0.0937\n",
      "Epoch 17/100\n",
      "146/146 [==============================] - 90s 618ms/step - loss: 0.2734 - accuracy: 0.9896 - precision: 0.9991 - recall: 0.7258 - auc: 0.9980 - val_loss: 2.1821 - val_accuracy: 0.4444 - val_precision: 0.7000 - val_recall: 0.0236 - val_auc: 0.3893\n",
      "Epoch 18/100\n",
      "146/146 [==============================] - 90s 619ms/step - loss: 0.2594 - accuracy: 0.9912 - precision: 0.9998 - recall: 0.6522 - auc: 0.9968 - val_loss: 1.4653 - val_accuracy: 0.6465 - val_precision: 0.5000 - val_recall: 0.0101 - val_auc: 0.6663\n",
      "Epoch 19/100\n",
      "146/146 [==============================] - 90s 618ms/step - loss: 0.2696 - accuracy: 0.9844 - precision: 0.9979 - recall: 0.6575 - auc: 0.9946 - val_loss: 1.8091 - val_accuracy: 0.6094 - val_precision: 0.6389 - val_recall: 0.0774 - val_auc: 0.6244\n",
      "Epoch 20/100\n",
      "146/146 [==============================] - 90s 619ms/step - loss: 0.2519 - accuracy: 0.9899 - precision: 0.9992 - recall: 0.7273 - auc: 0.9975 - val_loss: 0.8966 - val_accuracy: 0.8586 - val_precision: 0.8333 - val_recall: 0.0505 - val_auc: 0.8456\n",
      "Epoch 21/100\n",
      "146/146 [==============================] - 90s 618ms/step - loss: 0.2491 - accuracy: 0.9900 - precision: 0.9986 - recall: 0.7077 - auc: 0.9971 - val_loss: 2.9493 - val_accuracy: 0.3502 - val_precision: 0.3529 - val_recall: 0.0202 - val_auc: 0.2942\n",
      "Epoch 22/100\n",
      "146/146 [==============================] - 90s 617ms/step - loss: 0.2239 - accuracy: 0.9970 - precision: 0.9998 - recall: 0.7341 - auc: 0.9986 - val_loss: 0.9495 - val_accuracy: 0.7576 - val_precision: 0.7778 - val_recall: 0.0236 - val_auc: 0.7899\n",
      "Epoch 23/100\n",
      "146/146 [==============================] - 90s 618ms/step - loss: 0.2246 - accuracy: 0.9938 - precision: 1.0000 - recall: 0.6462 - auc: 0.9962 - val_loss: 0.8627 - val_accuracy: 0.8620 - val_precision: 0.8333 - val_recall: 0.0337 - val_auc: 0.8292\n",
      "Epoch 24/100\n",
      "146/146 [==============================] - 90s 618ms/step - loss: 0.2644 - accuracy: 0.9825 - precision: 0.9950 - recall: 0.7900 - auc: 0.9939 - val_loss: 1.4517 - val_accuracy: 0.6162 - val_precision: 0.8276 - val_recall: 0.0808 - val_auc: 0.6715\n",
      "Epoch 25/100\n",
      "146/146 [==============================] - 90s 617ms/step - loss: 0.2371 - accuracy: 0.9892 - precision: 0.9996 - recall: 0.7400 - auc: 0.9973 - val_loss: 0.7402 - val_accuracy: 0.8283 - val_precision: 0.8000 - val_recall: 0.0269 - val_auc: 0.8460\n",
      "Epoch 26/100\n",
      "146/146 [==============================] - 90s 617ms/step - loss: 0.2463 - accuracy: 0.9862 - precision: 0.9989 - recall: 0.7382 - auc: 0.9957 - val_loss: 0.9572 - val_accuracy: 0.7576 - val_precision: 0.8500 - val_recall: 0.0572 - val_auc: 0.8023\n",
      "Epoch 27/100\n",
      "146/146 [==============================] - 90s 617ms/step - loss: 0.2330 - accuracy: 0.9890 - precision: 0.9995 - recall: 0.7555 - auc: 0.9975 - val_loss: 1.7710 - val_accuracy: 0.5960 - val_precision: 0.7500 - val_recall: 0.0404 - val_auc: 0.6215\n",
      "Epoch 28/100\n",
      "146/146 [==============================] - 90s 616ms/step - loss: 0.2351 - accuracy: 0.9879 - precision: 0.9983 - recall: 0.8000 - auc: 0.9972 - val_loss: 3.8680 - val_accuracy: 0.3199 - val_precision: 0.2000 - val_recall: 0.0572 - val_auc: 0.2436\n",
      "Epoch 29/100\n",
      "146/146 [==============================] - 90s 616ms/step - loss: 0.2211 - accuracy: 0.9910 - precision: 0.9988 - recall: 0.8314 - auc: 0.9984 - val_loss: 0.7874 - val_accuracy: 0.8215 - val_precision: 0.9688 - val_recall: 0.1044 - val_auc: 0.8767\n",
      "Epoch 30/100\n",
      "146/146 [==============================] - 90s 617ms/step - loss: 0.2189 - accuracy: 0.9915 - precision: 0.9998 - recall: 0.7521 - auc: 0.9978 - val_loss: 2.3535 - val_accuracy: 0.4714 - val_precision: 0.5000 - val_recall: 0.0303 - val_auc: 0.4782\n",
      "Epoch 31/100\n",
      "146/146 [==============================] - 90s 617ms/step - loss: 0.2151 - accuracy: 0.9925 - precision: 0.9998 - recall: 0.7314 - auc: 0.9976 - val_loss: 1.1766 - val_accuracy: 0.7374 - val_precision: 0.8421 - val_recall: 0.0539 - val_auc: 0.7650\n",
      "Epoch 32/100\n",
      "146/146 [==============================] - 90s 616ms/step - loss: 0.2215 - accuracy: 0.9900 - precision: 0.9994 - recall: 0.7456 - auc: 0.9967 - val_loss: 0.8435 - val_accuracy: 0.8047 - val_precision: 0.9571 - val_recall: 0.2256 - val_auc: 0.8603\n",
      "Epoch 33/100\n",
      "146/146 [==============================] - 90s 616ms/step - loss: 0.2138 - accuracy: 0.9916 - precision: 0.9997 - recall: 0.8033 - auc: 0.9984 - val_loss: 0.8238 - val_accuracy: 0.8586 - val_precision: 0.9524 - val_recall: 0.2020 - val_auc: 0.8923\n",
      "Epoch 34/100\n",
      "146/146 [==============================] - 90s 615ms/step - loss: 0.2128 - accuracy: 0.9919 - precision: 0.9993 - recall: 0.7541 - auc: 0.9967 - val_loss: 3.2621 - val_accuracy: 0.3434 - val_precision: 0.2545 - val_recall: 0.0471 - val_auc: 0.3137\n",
      "Epoch 35/100\n",
      "146/146 [==============================] - 90s 617ms/step - loss: 0.2148 - accuracy: 0.9889 - precision: 0.9989 - recall: 0.8370 - auc: 0.9984 - val_loss: 1.1902 - val_accuracy: 0.7205 - val_precision: 0.9400 - val_recall: 0.1582 - val_auc: 0.7668\n",
      "Epoch 36/100\n",
      "146/146 [==============================] - 90s 616ms/step - loss: 0.1944 - accuracy: 0.9956 - precision: 1.0000 - recall: 0.7847 - auc: 0.9982 - val_loss: 0.7706 - val_accuracy: 0.8754 - val_precision: 0.9355 - val_recall: 0.0976 - val_auc: 0.8626\n",
      "Epoch 37/100\n",
      "100/146 [===================>..........] - ETA: 27s - loss: 0.2031 - accuracy: 0.9934 - precision: 0.9997 - recall: 0.7082 - auc: 0.9962"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-3e9bb96fddb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m     ) #validation_data=(x_test, y_test),batch_size = 10, \n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# from tensorflow.python.client import device_lib\n",
    "# print(device_lib.list_local_devices())\n",
    "# print(tf.__version__)\n",
    "# print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "# tf.test.is_gpu_available()\n",
    "\n",
    "\n",
    "\n",
    "class datagenator(tf.keras.utils.Sequence):\n",
    "\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, batch_size=10, shuffle=True,n_classes = 2,mode='train'):\n",
    "        'Initialization'\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.shuffle = shuffle\n",
    "        self.n_classes = n_classes\n",
    "        self.mode = mode\n",
    "        self.err={\n",
    "            'effective':[],\n",
    "            'invalid':[]\n",
    "        }\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "        # print(80*'!')\n",
    "        # print(np.shape(X),np.shape(y))\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = [] #np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = [] #np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for ID in list_IDs_temp:\n",
    "            # Store sample\n",
    "#             print(ID)\n",
    "            img = cv2.imread(ID)\n",
    "            in_ = np.array(img, dtype=np.float32)\n",
    "            shape = np.shape(in_)\n",
    "#             print(shape)\n",
    "#             try:\n",
    "            side = 128\n",
    "            if(shape[0]>shape[1]):\n",
    "                    in_=cv2.resize(in_,(side,shape[1]*side/shape[0]),interpolation=cv2.INTER_CUBIC)\n",
    "                    newshape=np.shape(in_)\n",
    "                    addsp=(side-newshape[0])/2\n",
    "                    fr_sp=np.zeros((addsp,side,3),np.float32)\n",
    "                    if((side-newshape[0])%2!=0):\n",
    "                            addsp+=1\n",
    "                    bk_sp=np.zeros((addsp,side,3),np.float32)\n",
    "                    in_=np.concatenate((fr_sp,in_,bk_sp),axis=0)\n",
    "            elif(shape[0]<shape[1]):\n",
    "                    in_=cv2.resize(in_,(shape[0]*side/shape[1],side),interpolation=cv2.INTER_CUBIC)\n",
    "                    newshape=np.shape(in_)\n",
    "                    addsp=(side-newshape[1])/2\n",
    "                    fr_sp=np.zeros((side,addsp,3),np.float32)\n",
    "                    if((side-newshape[1])%2!=0):\n",
    "                            addsp+=1\n",
    "                    bk_sp=np.zeros((side,addsp,3),np.float32)\n",
    "                    in_=np.concatenate((fr_sp,in_,bk_sp),axis=1)\n",
    "\n",
    "            else:\n",
    "                    in_=cv2.resize(in_,(side,side),interpolation=cv2.INTER_CUBIC)\n",
    "#             except:\n",
    "#                 in_ = np.array(np.random.randint(0,256,size = (517,517,3)), dtype=np.float32)\n",
    "#                 # print(ID)\n",
    "#                 if('effective' in ID):\n",
    "#                     if not(ID.split('effective')[-1].split('/')[1] in self.err['effective']):\n",
    "#                         self.err['effective'].append(ID.split('effective')[-1].split('/')[1])\n",
    "#                         self.err['effective'].sort()\n",
    "#                 if('invalid' in ID):\n",
    "#                     if not(ID.split('invalid')[-1].split('/')[1] in self.err['invalid']):\n",
    "#                         self.err['invalid'].append(ID.split('invalid')[-1].split('/')[1])\n",
    "#                         self.err['invalid'].sort()\n",
    "\n",
    "#                 if(self.mode =='train'):\n",
    "#                     with open('err_train.json','w') as f:\n",
    "#                         # f.writelines(ID + '\\n')\n",
    "#                         json.dump(self.err,f,indent=4)\n",
    "                \n",
    "#                 if(self.mode =='val'):\n",
    "#                     with open('err_test.json','w') as f:\n",
    "#                         # f.writelines(ID + '\\n')\n",
    "#                         json.dump(self.err,f,indent=4)\n",
    "            \n",
    "            X.append(in_)\n",
    "\n",
    "            # Store class\n",
    "            y.append(self.labels[ID])\n",
    "\n",
    "        return np.array(X), np.array(to_categorical(y, num_classes=self.n_classes))\n",
    "\n",
    "\n",
    "imgList = json.load(open('/home/edward/test/OUTtest/CMU_AICenter/trainReal1.json'))\n",
    "\n",
    "partition = {\n",
    "    'train' :[],\n",
    "    'test' :[]\n",
    "}\n",
    "labels = {}\n",
    "\n",
    "for i in imgList['train']:\n",
    "    partition['train'].append(i[0])\n",
    "    labels[i[0]] = i[1]\n",
    "for i in imgList['test']:\n",
    "    partition['test'].append(i[0])\n",
    "    labels[i[0]] = i[1]\n",
    "\n",
    "# this could also be the output a different Keras model or layer\n",
    "# input_tensor = Input(shape=(517, 517, 3))\n",
    "# # create the base pre-trained model\n",
    "# base_model = InceptionV3(   \n",
    "#     input_tensor=input_tensor,\n",
    "#     weights=None,\n",
    "#     include_top=True,\n",
    "#     classes=2,\n",
    "# ) #input_tensor=input_tensor, weights='imagenet', \n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "# x = base_model.output\n",
    "# x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "# x = Dense(1024, activation='relu')(x)\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "# predictions = Dense(2, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = resnet_v2((128,128,3),(9*6)+2,2)#Model(inputs=base_model.input, outputs=x) resnet_v2((128,128,3),(9*6)+2,2)\n",
    "# model.summary()\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer='nadam', loss='categorical_crossentropy', metrics=['accuracy', 'Precision', 'Recall', 'AUC']) \n",
    "#'categorical_crossentropy' [binary_focal_loss(alpha=.25, gamma=2)] weighted_loss\n",
    "\n",
    "# model.load_weights(\"/home/edward/test/OUTtest/CMU_AICenter/RMS_focal/weights.30-0.96.hdf5\", by_name=True)\n",
    "\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "checkPointPath = './checkpoint/weights.{epoch:02d}-{auc:.2f}.hdf5'\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(checkPointPath, monitor='auc', save_weights_only=True)\n",
    "\n",
    "training_generator = datagenator(partition['train'], labels, batch_size=50, n_classes = 2, shuffle=True, mode='train')\n",
    "val_generator = datagenator(partition['test'], labels, batch_size=9, n_classes = 2, shuffle=True,mode='val')\n",
    "# x_train = np.random.randint(0,256,size=(1000,517,517,3))\n",
    "# print(np.shape(x_train))\n",
    "# y_train=to_categorical(np.random.randint(0,2,size=(1000)))\n",
    "# print('y_train= ',y_train)\n",
    "# print([[0.,1.]])\n",
    "\n",
    "model.fit(\n",
    "    x = training_generator, \n",
    "    epochs = 100,\n",
    "    callbacks = [tensorboard_callback, checkpoint_callback],\n",
    "    validation_data = val_generator\n",
    "    ) #validation_data=(x_test, y_test),batch_size = 10, \n",
    "\n",
    "log_writer = tf.summary.create_file_writer(logdir)\n",
    "\n",
    "model.save_weights(os.path.join(\"weights\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + '.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "# from tensorflowf.keras.utils import Sequence\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.models import Model\n",
    "from keras import backend as K\n",
    "import numpy as np,cv2,tensorflow as tf,json,os,datetime\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import  Conv2D, BatchNormalization, Activation, Input, AveragePooling2D, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_pos = K.constant([3993/4242],dtype='float32')\n",
    "weights_neg = K.constant([249/4242],dtype='float32')\n",
    "epsilon=1e-7\n",
    "def weighted_loss(y_true, y_pred):\n",
    "    loss = 0.0\n",
    "    loss_pos = -1 * K.sum( weights_pos * y_true * K.log(y_pred + epsilon), axis=-1)\n",
    "    loss_neg = -1 * K.sum( weights_neg * (1 - y_true) * K.log(1 - y_pred + epsilon) ,axis=-1)\n",
    "    return (loss_pos+loss_neg)/2\n",
    "\n",
    "def binary_focal_loss(gamma=2, alpha=0.25):\n",
    "    alpha = tf.constant(alpha, dtype=tf.float32)\n",
    "    gamma = tf.constant(gamma, dtype=tf.float32)\n",
    "\n",
    "    def binary_focal_loss_fixed(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        alpha_t = y_true*alpha + (K.ones_like(y_true)-y_true)*(1-alpha)\n",
    "    \n",
    "        p_t = y_true*y_pred + (K.ones_like(y_true)-y_true)*(K.ones_like(y_true)-y_pred) + K.epsilon()\n",
    "        focal_loss = - alpha_t * K.pow((K.ones_like(y_true)-p_t),gamma) * K.log(p_t)\n",
    "        return K.mean(focal_loss)\n",
    "    return binary_focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_layer(inputs,\n",
    "                 num_filters=16,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 batch_normalization=True,\n",
    "                 conv_first=True):\n",
    "    \n",
    "    conv = Conv2D(num_filters,\n",
    "                  kernel_size=kernel_size,\n",
    "                  strides=strides,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))\n",
    "\n",
    "    x = inputs\n",
    "    if conv_first:\n",
    "        x = conv(x)\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "    else:\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "        x = conv(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_v2(input_shape, depth,num_classes):\n",
    "    if (depth - 2) % 9 != 0:\n",
    "        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
    "    # Start model definition.\n",
    "    num_filters_in = 16\n",
    "    num_res_blocks = int((depth - 2) / 9)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
    "    x = resnet_layer(inputs=inputs,\n",
    "                     num_filters=num_filters_in,\n",
    "                     conv_first=True)\n",
    "\n",
    "    # Instantiate the stack of residual units\n",
    "    for stage in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            activation = 'relu'\n",
    "            batch_normalization = True\n",
    "            strides = 1\n",
    "            if stage == 0:\n",
    "                num_filters_out = num_filters_in * 4\n",
    "                if res_block == 0:  # first layer and first stage\n",
    "                    activation = None\n",
    "                    batch_normalization = False\n",
    "            else:\n",
    "                num_filters_out = num_filters_in * 2\n",
    "                if res_block == 0:  # first layer but not first stage\n",
    "                    strides = 2    # downsample\n",
    "\n",
    "            # bottleneck residual unit\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters_in,\n",
    "                             kernel_size=1,\n",
    "                             strides=strides,\n",
    "                             activation=activation,\n",
    "                             batch_normalization=batch_normalization,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_in,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_out,\n",
    "                             kernel_size=1,\n",
    "                             conv_first=False)\n",
    "            if res_block == 0:\n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters_out,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = tf.keras.layers.add([x, y])\n",
    "\n",
    "        num_filters_in = num_filters_out\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v2 has BN-ReLU before Pooling\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu',name='last_activation_layer')(x)\n",
    "    x = AveragePooling2D(pool_size=8)(x)\n",
    "    y = Flatten()(x)\n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='sigmoid',\n",
    "                    kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # Instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class datagenator(tf.keras.utils.Sequence):\n",
    "\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, batch_size=10, shuffle=True,n_classes = 10,mode='train'):\n",
    "        'Initialization'\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.shuffle = shuffle\n",
    "        self.n_classes = n_classes\n",
    "        self.mode = mode\n",
    "        self.err={\n",
    "            'effective':[],\n",
    "            'invalid':[]\n",
    "        }\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "#         print(len(self.list_IDs))\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "        # print(80*'!')\n",
    "        # print(np.shape(X),np.shape(y))\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = [] #np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = [] #np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for ID in list_IDs_temp:\n",
    "            # Store sample\n",
    "            img = cv2.imread(ID)\n",
    "            in_ = np.array(img, dtype=np.float32)\n",
    "            shape=np.shape(in_)\n",
    "            try:\n",
    "                side = 128\n",
    "                if(shape[0]>shape[1]):\n",
    "                        in_=cv2.resize(in_,(side,shape[1]*side/shape[0]),interpolation=cv2.INTER_CUBIC)\n",
    "                        newshape=np.shape(in_)\n",
    "                        addsp=(side-newshape[0])/2\n",
    "                        fr_sp=np.zeros((addsp,side,3),np.float32)\n",
    "                        if((side-newshape[0])%2!=0):\n",
    "                                addsp+=1\n",
    "                        bk_sp=np.zeros((addsp,side,3),np.float32)\n",
    "                        in_=np.concatenate((fr_sp,in_,bk_sp),axis=0)\n",
    "                elif(shape[0]<shape[1]):\n",
    "                        in_=cv2.resize(in_,(shape[0]*side/shape[1],side),interpolation=cv2.INTER_CUBIC)\n",
    "                        newshape=np.shape(in_)\n",
    "                        addsp=(side-newshape[1])/2\n",
    "                        fr_sp=np.zeros((side,addsp,3),np.float32)\n",
    "                        if((side-newshape[1])%2!=0):\n",
    "                                addsp+=1\n",
    "                        bk_sp=np.zeros((side,addsp,3),np.float32)\n",
    "                        in_=np.concatenate((fr_sp,in_,bk_sp),axis=1)\n",
    "\n",
    "                else:\n",
    "                        in_=cv2.resize(in_,(side,side),interpolation=cv2.INTER_CUBIC)\n",
    "            except:\n",
    "                in_ = np.array(np.random.randint(0,256,size = (517,517,3)), dtype=np.float32)\n",
    "                # print(ID)\n",
    "                if('effective' in ID):\n",
    "                    if not(ID.split('effective')[-1].split('/')[1] in self.err['effective']):\n",
    "                        self.err['effective'].append(ID.split('effective')[-1].split('/')[1])\n",
    "                        self.err['effective'].sort()\n",
    "                if('invalid' in ID):\n",
    "                    if not(ID.split('invalid')[-1].split('/')[1] in self.err['invalid']):\n",
    "                        self.err['invalid'].append(ID.split('invalid')[-1].split('/')[1])\n",
    "                        self.err['invalid'].sort()\n",
    "\n",
    "                if(self.mode =='train'):\n",
    "                    with open('err_train.json','w') as f:\n",
    "                        # f.writelines(ID + '\\n')\n",
    "                        json.dump(self.err,f,indent=4)\n",
    "                \n",
    "                if(self.mode =='val'):\n",
    "                    with open('err_test.json','w') as f:\n",
    "                        # f.writelines(ID + '\\n')\n",
    "                        json.dump(self.err,f,indent=4)\n",
    "            \n",
    "            X.append(in_)\n",
    "\n",
    "            # Store class\n",
    "            y.append(self.labels[ID])\n",
    "\n",
    "        return np.array(X), np.array(to_categorical(y, num_classes=self.n_classes))\n",
    "\n",
    "imgList = json.load(open('/home/edward/test/OUTtest/CMU_AICenter/trainReal1.json'))\n",
    "\n",
    "partition = {\n",
    "    'train':[],\n",
    "    'test' :[]\n",
    "}\n",
    "labels = {}\n",
    "\n",
    "# print(len(imgList['test']))\n",
    "for i in imgList['train']:\n",
    "    if not('output' in i[0]):\n",
    "        partition['train'].append(i[0])\n",
    "        labels[i[0]] = i[1]\n",
    "for i in imgList['test']:\n",
    "    partition['test'].append(i[0])\n",
    "    labels[i[0]] = i[1]\n",
    "\n",
    "train__generator = datagenator(partition['train'], labels, batch_size=6, n_classes = 2, shuffle=False, mode='train')\n",
    "val__generator = datagenator(partition['test'], labels, batch_size=9, n_classes = 2, shuffle=False, mode='val')\n",
    "\n",
    "# input_tensor = Input(shape=(517, 517, 3))\n",
    "# base_model = InceptionV3(   \n",
    "#     input_tensor=input_tensor,\n",
    "#     weights=None,\n",
    "#     include_top=True,\n",
    "#     classes=2,\n",
    "# )\n",
    "# x = base_model.output\n",
    "\n",
    "model = resnet_v2((128,128,3),(9*6)+2,2) #Model(inputs=base_model.input, outputs=x)\n",
    "model.compile(optimizer='nadam', loss='categorical_crossentropy', metrics=['accuracy', 'Precision', 'Recall', 'AUC'])\n",
    "#'categorical_crossentropy' [binary_focal_loss(alpha=.25, gamma=2)] weighted_loss\n",
    "\n",
    "# model.load_weights(\"/home/edward/test/OUTtest/CMU_AICenter/checkpoint/weights.100-0.96.hdf5\", by_name=True)\n",
    "\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4242/4242 [==============================] - 52s 12ms/step - loss: 0.0448 - accuracy: 0.9413 - precision: 0.7775 - recall: 0.9736 - auc: 0.9638\n",
      "{'loss': 0.04483475536108017, 'accuracy': 0.9413012862205505, 'precision': 0.7774849534034729, 'recall': 0.9735973477363586, 'auc': 0.9637787342071533}\n"
     ]
    }
   ],
   "source": [
    "#==============evaluate======================\n",
    "evalValue = model.evaluate(\n",
    "    x=val__generator,\n",
    "    callbacks=tensorboard_callback,\n",
    "    return_dict=True,\n",
    ")\n",
    "print(evalValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(297, 2)\n"
     ]
    }
   ],
   "source": [
    "#==============predict=======================\n",
    "value = model.predict(\n",
    "    x=val__generator,\n",
    "    callbacks=tensorboard_callback,\n",
    ") #use_multiprocessing=True, \n",
    "\n",
    "# print(value,np.shape(value))\n",
    "print(np.shape(value))\n",
    "\n",
    "predictDict={}\n",
    "\n",
    "for i in range(len(partition['test'])):\n",
    "    predictDict[partition['test'][i]]=value[i].tolist()\n",
    "#     print(partition['test'][i],np.argmax(value[i]))\n",
    "\n",
    "with open('predict1.json','w') as f:\n",
    "    json.dump(predictDict,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01 249 1 3992 0\n",
      "02 227 1469 2524 22\n",
      "03 14 3984 9 235\n",
      "04 242 275 3718 7\n",
      "05 119 3899 94 130\n",
      "06 183 3959 34 66\n",
      "07 247 3050 943 2\n",
      "08 215 2954 1039 34\n",
      "09 222 3941 52 27\n",
      "10 247 2627 1366 2\n",
      "11 232 3876 117 17\n",
      "12 40 3988 5 209\n",
      "13 134 3986 7 115\n",
      "14 241 3910 83 8\n",
      "15 248 3566 427 1\n",
      "16 249 22 3971 0\n",
      "17 223 3911 82 26\n",
      "18 248 3248 745 1\n",
      "19 248 2659 1334 1\n"
     ]
    }
   ],
   "source": [
    "modelRoot = \"/home/edward/test/OUTtest/CMU_AICenter/checkpoint/\"\n",
    "modelList = os.listdir(modelRoot)\n",
    "modelList.sort()\n",
    "for modelPath in modelList:\n",
    "#     print(modelPath,modelPath.split('.')[1].split('-')[0])\n",
    "    model.load_weights(modelRoot + modelPath, by_name=True)\n",
    "    value = model.predict(\n",
    "        x=train__generator,\n",
    "        callbacks=tensorboard_callback,\n",
    "    )\n",
    "    predictDict={}\n",
    "    for i in range(len(partition['train'])):\n",
    "        predictDict[partition['train'][i]]=value[i].tolist()\n",
    "    errCount = [0,0,0,0] #TP,TN,FP,FN\n",
    "    for k,v in predictDict.items(): \n",
    "        pre = np.argmax(v)\n",
    "        real = labels[k]\n",
    "#         print(k,real, pre)\n",
    "    #         if(pre != real):\n",
    "    #             print(np.argmax(v),labels[k])\n",
    "        if(real == 1):\n",
    "            if(pre == 1):\n",
    "                errCount[0] += 1\n",
    "            else:\n",
    "                errCount[3] += 1\n",
    "        else:\n",
    "            if(pre == 1):\n",
    "                errCount[2] += 1\n",
    "            else:\n",
    "                errCount[1] += 1\n",
    "\n",
    "    print(modelPath.split('.')[1].split('-')[0],errCount[0],errCount[1],errCount[2],errCount[3])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
